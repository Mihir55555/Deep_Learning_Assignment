{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backward Propagation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of forward propagation in a neural network.\n",
    "\n",
    "ANS:\n",
    "\n",
    "Forward propagation is the process through which input data passes through the neural network layer by layer to produce an output. In this process, the network computes the weighted sum of inputs at each layer and applies an activation function to generate outputs for the next layer.\n",
    "\n",
    "The steps involved in forward propagation are:\n",
    "\n",
    "Input Layer: The input data is fed into the neural network. Weight Multiplication: The input is multiplied by the weights associated with each connection between neurons. Bias Addition: A bias term is added to the weighted input. Activation Function: The result of the weighted input plus bias is passed through an activation function, which introduces non-linearity to the model. Repeat for Each Layer: These steps are repeated for every hidden layer. Output Layer: In the final layer, the network computes the output using the final set of weights and activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the purpose of the activation function in forward propagation\n",
    "\n",
    "ANS:\n",
    "\n",
    "The activation function introduces non-linearity to the model, which is crucial for the neural network to learn and model complex patterns. Without activation functions, the network would simply be a linear transformation of the input, which limits its capacity to solve non-linear problems.\n",
    "\n",
    "Linear Activation: No non-linearity, meaning the output is simply a weighted sum of inputs. Non-Linear Activation: Functions like ReLU, Sigmoid, or Tanh add non-linearity, allowing the model to approximate complex functions and capture intricate patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Describe the steps involved in the backward propagation (backpropagation) algorithm'\n",
    "\n",
    "ANS:\n",
    "\n",
    "Backpropagation is the process of updating the weights of a neural network based on the error of the output. It computes the gradient of the loss function with respect to each weight by applying the chain rule of calculus.\n",
    "\n",
    "The key steps in backpropagation are:\n",
    "\n",
    "Compute the Loss: Calculate the loss (or error) between the predicted output and the actual target using a loss function (e.g., Mean Squared Error, Cross-Entropy).\n",
    "\n",
    "Calculate the Gradient of the Loss: For each layer, calculate the gradient of the loss function with respect to the weights and biases. This involves:\n",
    "\n",
    "Output Layer: Compute the gradient of the loss with respect to the output of the layer and then backpropagate through the activation function. Hidden Layers: For each hidden layer, backpropagate the gradient to the previous layer by calculating the partial derivative of the loss with respect to the layerâ€™s weights. Weight Update: Use the computed gradients to update the weights by adjusting them in the direction that minimizes the loss. Typically, a learning rate is used to control the size of the weight update.\n",
    "\n",
    "Repeat: Repeat this process for each batch of data until the loss converges to a low value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the purpose of the chain rule in backpropagation\n",
    "\n",
    "ANS:\n",
    "\n",
    "The chain rule is essential in backpropagation because it allows us to compute the gradient of the loss function with respect to each weight in the network. Since the output of each neuron depends on the weights of the previous layer, we use the chain rule to break down the derivative of the loss function into simpler parts. This enables us to efficiently propagate the error from the output layer back to the input layer, adjusting the weights at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Implement the forward propagation process for a simple neural network with one hidden layer usingNumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network:\n",
      "[[0.55563549]\n",
      " [0.48364651]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the activation function (Sigmoid in this case)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the input, weights, and biases for a simple network\n",
    "# Assume input layer has 3 neurons, hidden layer has 4 neurons, and output layer has 1 neuron\n",
    "\n",
    "# Input (batch size = 2, features = 3)\n",
    "X = np.array([[0.1, 0.2, 0.3], \n",
    "              [0.4, 0.5, 0.6]])\n",
    "\n",
    "# Weights for the hidden layer (3 input neurons, 4 hidden neurons)\n",
    "W_hidden = np.random.randn(3, 4)\n",
    "\n",
    "# Biases for the hidden layer (4 neurons in the hidden layer)\n",
    "b_hidden = np.random.randn(4)\n",
    "\n",
    "# Weights for the output layer (4 hidden neurons, 1 output neuron)\n",
    "W_output = np.random.randn(4, 1)\n",
    "\n",
    "# Bias for the output layer (1 output neuron)\n",
    "b_output = np.random.randn(1)\n",
    "\n",
    "# Forward Propagation\n",
    "# Step 1: Calculate the weighted sum for the hidden layer\n",
    "Z_hidden = np.dot(X, W_hidden) + b_hidden\n",
    "\n",
    "# Step 2: Apply activation function to the hidden layer (using Sigmoid)\n",
    "A_hidden = sigmoid(Z_hidden)\n",
    "\n",
    "# Step 3: Calculate the weighted sum for the output layer\n",
    "Z_output = np.dot(A_hidden, W_output) + b_output\n",
    "\n",
    "# Step 4: Apply activation function to the output (Sigmoid for a binary classification task)\n",
    "A_output = sigmoid(Z_output)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output of the network:\")\n",
    "print(A_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
